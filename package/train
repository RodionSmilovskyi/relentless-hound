#!/usr/bin/env python
# pylint: skip-file

import os
import sys
import settings
import numpy as np
import gym
import argparse
import time
import datetime
import tensorflow as tf

from gym.wrappers import FrameStack, RecordVideo
from typing import Optional

from envs.drone import FRAME_NUMBER, DroneEnv
from envs.cv_wrapper import CvWrapper
from envs.reward_wrapper import RewardWrapper

import reverb

from tf_agents.environments.gym_wrapper import GymWrapper
from tf_agents.agents.reinforce import reinforce_agent
from tf_agents.drivers import py_driver
from tf_agents.environments import tf_py_environment
from tf_agents.networks import actor_distribution_network
from tf_agents.policies import py_tf_eager_policy
from tf_agents.policies import policy_saver
from tf_agents.replay_buffers import reverb_replay_buffer
from tf_agents.replay_buffers import reverb_utils
from tf_agents.specs import tensor_spec
from tf_agents.trajectories import trajectory
from tf_agents.utils import common
from tf_agents.train.utils import strategy_utils

num_eval_episodes = 2  # @param {type:"integer"}
collect_episodes_per_iteration = 5


def compute_avg_return(environment, policy, num_episodes=10):
    total_return = 0.0

    for _ in range(num_episodes):
        time_step = environment.reset()
        episode_return = 0.0

        while not time_step.is_last():
            action_step = policy.action(time_step)
            time_step = environment.step(action_step.action)
            episode_return += time_step.reward
        total_return += episode_return

    avg_return = total_return / num_episodes
    return avg_return.numpy()[0]


def collect_episode(environment, policy, rb_observer, num_episodes):

    driver = py_driver.PyDriver(
        environment,
        py_tf_eager_policy.PyTFEagerPolicy(policy, use_tf_function=True),
        [rb_observer],
        max_episodes=num_episodes,
    )

    initial_time_step = environment.reset()
    driver.run(initial_time_step)


def train(use_gpu):
    train_py_env = GymWrapper(RewardWrapper(FrameStack(CvWrapper(DroneEnv(False)), 4)))
    train_env = tf_py_environment.TFPyEnvironment(train_py_env)
    train_step_counter = tf.Variable(0, dtype=tf.int64)

    eval_py_env = GymWrapper(
        RecordVideo(
            RewardWrapper(FrameStack(CvWrapper(DroneEnv(settings.USE_GUI)), 4)),
            f"{settings.OUTPUT_PATH}/data/videos",
            episode_trigger=lambda ep_id: ep_id % num_eval_episodes == 0,
            name_prefix=f"validation-{train_step_counter.numpy()}",
        )
    )
    eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)

    actor_net = actor_distribution_network.ActorDistributionNetwork(
        train_env.observation_spec(),
        train_env.action_spec(),
        fc_layer_params=settings.FC_LAYER_PARAMS,
    )

    optimizer = tf.keras.optimizers.Adam(learning_rate=settings.LEARNING_RATE)

    tf_agent = reinforce_agent.ReinforceAgent(
        train_env.time_step_spec(),
        train_env.action_spec(),
        actor_network=actor_net,
        optimizer=optimizer,
        normalize_returns=True,
        train_step_counter=train_step_counter,
    )

    tf_agent.initialize()

    replay_buffer_signature = tensor_spec.from_spec(tf_agent.collect_data_spec)
    replay_buffer_signature = tensor_spec.add_outer_dim(replay_buffer_signature)
    table_name = "uniform_table"
    table = reverb.Table(
        table_name,
        max_size=settings.BUFFER_SIZE,
        sampler=reverb.selectors.Uniform(),
        remover=reverb.selectors.Fifo(),
        rate_limiter=reverb.rate_limiters.MinSize(1),
        signature=replay_buffer_signature,
    )

    reverb_server = reverb.Server([table])

    replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(
        tf_agent.collect_data_spec,
        table_name=table_name,
        sequence_length=None,
        local_server=reverb_server,
    )

    train_checkpointer = common.Checkpointer(
        ckpt_dir=settings.CHECKPOINT_PATH,
        max_to_keep=1,
        tf_agent=tf_agent,
        replay_buffer=replay_buffer
    )
    train_checkpointer.initialize_or_restore()

    # rb_observer = reverb_utils.ReverbAddEpisodeObserver(
    #     replay_buffer.py_client, table_name, FRAME_NUMBER
    # )

    rb_observer = reverb_utils.ReverbAddTrajectoryObserver(
        replay_buffer.py_client, table_name, FRAME_NUMBER, pad_end_of_episodes=True
    )

    tf_agent.train = common.function(tf_agent.train)
    tf_agent.train_step_counter.assign(0)
    avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)

    tf.summary.scalar(
        "Average Return", data=avg_return, step=tf_agent.train_step_counter.numpy()
    )

    start_time = time.time()

    for _ in range(settings.EPISODES):
        collect_episode(
            train_py_env,
            tf_agent.collect_policy,
            rb_observer,
            collect_episodes_per_iteration,
        )

        iterator = iter(
            replay_buffer.as_dataset(sample_batch_size=collect_episodes_per_iteration)
        )
        trajectories, _ = next(iterator)
        train_loss = tf_agent.train(experience=trajectories)

        replay_buffer.clear()

        step = tf_agent.train_step_counter.numpy()
        print("step = {0}: loss = {1}".format(step, train_loss.loss))

        if step % settings.EPISODE_TRIGGER_STEP == 0:
            avg_return = compute_avg_return(
                eval_env, tf_agent.policy, num_eval_episodes
            )
            tf.summary.scalar("Average Return", data=avg_return, step=step)
            print("step = {0}: Average Return = {1}".format(step, avg_return))
            train_checkpointer.save(train_step_counter)

    rb_observer.close()
    reverb_server.stop()

    policy_dir = os.path.join(settings.OUTPUT_PATH, "policy")
    if not os.path.exists(settings.MODEL_PATH):
        os.makedirs(settings.MODEL_PATH)

    converter = tf.lite.TFLiteConverter.from_saved_model(
        policy_dir, signature_keys=["action"]
    )
    tflite_policy = converter.convert()

    with open(os.path.join(settings.MODEL_PATH, "policy.tflite"), "wb") as f:
        f.write(tflite_policy)

    end_time = time.time()
    elapsed_time = end_time - start_time

    print(f"Elapsed time: {elapsed_time} seconds")


def parse_tuple(s):
    parts = s.split("-")
    return tuple(map(int, parts))


if __name__ == "__main__":
    parser = argparse.ArgumentParser()

    parser.add_argument("--use-gui", action="store_true")
    parser.add_argument("--episodes", type=int, default=10)
    parser.add_argument(
        "--prefix", type=str, default=os.path.dirname(os.path.abspath(__file__))
    )
    parser.add_argument("--episode-trigger-step", type=int, default=5)
    parser.add_argument("--batch-size", type=int, default=32)
    parser.add_argument("--fc-layer-params", type=parse_tuple, default=(100,))
    parser.add_argument("--buffer-size", type=int, default=1000)
    parser.add_argument("--learning-rate", type=float, default=0.001)

    args = parser.parse_args()

    print(args)

    settings.INPUT_PATH = os.path.join(args.prefix, "input", "data")
    settings.OUTPUT_PATH = os.path.join(args.prefix, "output")
    settings.MODEL_PATH = os.path.join(args.prefix, "model")
    settings.CHECKPOINT_PATH = os.path.join(args.prefix, "checkpoints")
    settings.USE_GUI = args.use_gui
    settings.EPISODES = args.episodes
    settings.EPISODE_TRIGGER_STEP = args.episode_trigger_step
    settings.BATCH_SIZE = args.batch_size
    settings.FC_LAYER_PARAMS = args.fc_layer_params
    settings.BUFFER_SIZE = args.buffer_size
    settings.LEARNING_RATE = args.learning_rate

    train_file_writer = tf.summary.create_file_writer(
        settings.OUTPUT_PATH + "/tensorboard/train",
        flush_millis=1000
    )
    train_file_writer.set_as_default()

    # tf.debugging.experimental.enable_dump_debug_info(
    #     settings.OUTPUT_PATH + "/tensorboard",
    #     tensor_debug_mode="FULL_HEALTH",
    #     circular_buffer_size=-1,
    # )

    print("Num GPUs Available: ", len(tf.config.list_physical_devices("GPU")))

    train(len(tf.config.list_physical_devices("GPU")) > 0)

    print("SUCCESS")

    sys.exit(0)
