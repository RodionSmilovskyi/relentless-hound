#!/usr/bin/env python
# pylint: skip-file

import os
import sys
import settings
import argparse
import time
import functools
import tensorflow as tf

from absl import app
from absl import flags
from gym.wrappers import RecordVideo
from envs.drone import FRAME_NUMBER, DroneEnv
from envs.cv_wrapper import CvWrapper
from envs.hover_reward_wrapper import HowerRewardWrapper as RewardWrapper

from tf_agents.replay_buffers import tf_uniform_replay_buffer
from tf_agents.environments.gym_wrapper import GymWrapper
from tf_agents.agents.ppo import ppo_clip_agent
from tf_agents.metrics import tf_metrics
from tf_agents.eval import metric_utils
from tf_agents.drivers import dynamic_episode_driver
from tf_agents.environments import parallel_py_environment
from tf_agents.environments import tf_py_environment
from tf_agents.networks import actor_distribution_network, value_network
from tf_agents.policies import policy_saver
from tf_agents.utils import common
from tf_agents.system import system_multiprocessing as multiprocessing

class ParallelTrainingEnv(GymWrapper):
    def __init__(self):
        super().__init__(RewardWrapper(CvWrapper(DroneEnv(False))))

def parse_tuple(s):
    parts = s.split("-")
    return tuple(map(int, parts))

def collect_episode(environment, policy, observers, num_episodes):
    driver = dynamic_episode_driver.DynamicEpisodeDriver(
        environment,
        policy,
        observers=observers,
        num_episodes=num_episodes,
    )

    initial_time_step = environment.reset()
    driver.run(initial_time_step)
    
def print_settings():
    attributes = dir(settings)
    constants = [attr for attr in attributes if attr.isupper()]

    for constant in constants:
        print(f"{constant}: {getattr(settings, constant)}")

flags.DEFINE_boolean("use_gui", False, "use gui")
flags.DEFINE_string("prefix", os.path.dirname(os.path.abspath(__file__)), "main path")
flags.DEFINE_integer("number_of_parallel_envs", 5, "number of parallel envs")
flags.DEFINE_integer("validation_episode", 5, "validation episode")
flags.DEFINE_integer("num_eval_episodes", 30, "number of evaluation episodes")
flags.DEFINE_integer("episodes", 10, "number of episodes")
flags.DEFINE_float("learning_rate", 0.001, "Learning rate")
flags.DEFINE_integer(
    "collect_episodes_per_iteration", 10, "collect-episodes-per-iteration"
)
flags.DEFINE_string('fc_layer_params', "50-50", 'Fully connected layers parameters')


def train(_):
    settings.INPUT_PATH = os.path.join(flags.FLAGS.prefix, "input", "data")
    settings.OUTPUT_PATH = os.path.join(flags.FLAGS.prefix, "output")
    settings.MODEL_PATH = os.path.join(flags.FLAGS.prefix, "model")
    settings.CHECKPOINT_PATH = os.path.join(flags.FLAGS.prefix, "checkpoints")
    settings.USE_GUI = flags.FLAGS.use_gui
    settings.EPISODES = flags.FLAGS.episodes
    settings.NUM_EVAL_EPISODES = flags.FLAGS.num_eval_episodes
    settings.NUM_PARALLEL_ENV = flags.FLAGS.number_of_parallel_envs
    settings.VALIDATION_EPISODE = flags.FLAGS.validation_episode
    settings.COLLECT_EPISODES_PER_ITERATION = flags.FLAGS.collect_episodes_per_iteration
    settings.LEARNING_RATE = flags.FLAGS.learning_rate
    settings.FC_LAYER_PARAMS = parse_tuple(flags.FLAGS.fc_layer_params)
    
    print_settings()

    train_step_counter = tf.Variable(0, dtype=tf.int64)

    train_env = tf_py_environment.TFPyEnvironment(
        parallel_py_environment.ParallelPyEnvironment([ParallelTrainingEnv] * settings.NUM_PARALLEL_ENV)
    )
    train_file_writer = tf.summary.create_file_writer(
        settings.OUTPUT_PATH + "/tensorboard/train", flush_millis=1000
    )
    train_file_writer.set_as_default()

    eval_py_env = GymWrapper(
        RecordVideo(
            RewardWrapper(CvWrapper(DroneEnv(settings.USE_GUI))),
            f"{settings.OUTPUT_PATH}/data/videos",
            episode_trigger=lambda ep_id: ep_id % settings.NUM_EVAL_EPISODES == 0,
            name_prefix=f"validation-{train_step_counter.numpy()}",
        )
    )
    eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)
    eval_summary_writer = tf.compat.v2.summary.create_file_writer(
        settings.OUTPUT_PATH + "/tensorboard/eval", flush_millis=1000
    )

    actor_net = actor_distribution_network.ActorDistributionNetwork(
        train_env.observation_spec(),
        train_env.action_spec(),
        fc_layer_params=settings.FC_LAYER_PARAMS,
        activation_fn=tf.keras.activations.tanh,
    )
    value_net = value_network.ValueNetwork(
        train_env.observation_spec(),
        fc_layer_params=settings.FC_LAYER_PARAMS,
        activation_fn=tf.keras.activations.tanh,
    )

    optimizer = tf.keras.optimizers.Adam(learning_rate=settings.LEARNING_RATE)

    tf_agent = ppo_clip_agent.PPOClipAgent(
        train_env.time_step_spec(),
        train_env.action_spec(),
        optimizer,
        actor_net=actor_net,
        value_net=value_net,
        entropy_regularization=0.0,
        importance_ratio_clipping=0.2,
        normalize_observations=False,
        normalize_rewards=False,
        use_gae=True,
        num_epochs=25,
        train_step_counter=train_step_counter,
    )

    tf_agent.initialize()

    environment_steps_metric = tf_metrics.EnvironmentSteps()
    step_metrics = [
        tf_metrics.NumberOfEpisodes(),
        environment_steps_metric,
    ]
    train_metrics = step_metrics + [
        tf_metrics.AverageReturnMetric(batch_size=settings.NUM_PARALLEL_ENV),
        tf_metrics.AverageEpisodeLengthMetric(batch_size=settings.NUM_PARALLEL_ENV),
    ]

    eval_policy = tf_agent.policy
    collect_policy = tf_agent.collect_policy

    eval_metrics = [
        tf_metrics.AverageReturnMetric(buffer_size=settings.NUM_EVAL_EPISODES),
        tf_metrics.AverageEpisodeLengthMetric(buffer_size=settings.NUM_EVAL_EPISODES),
    ]

    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(
        tf_agent.collect_data_spec,
        batch_size=settings.NUM_PARALLEL_ENV,
        max_length=FRAME_NUMBER,
    )
    
    tf_agent.train_step_counter.assign(0)

    train_checkpointer = common.Checkpointer(
        ckpt_dir=settings.CHECKPOINT_PATH,
        max_to_keep=1,
        tf_agent=tf_agent,
        train_step_counter=train_step_counter,
        replay_buffer=replay_buffer,
    )
    train_checkpointer.initialize_or_restore()

    if train_checkpointer.checkpoint_exists == True:
        print("Checkpoint uploaded")

    tf_policy_saver = policy_saver.PolicySaver(eval_policy)
    policy_dir = os.path.join(settings.OUTPUT_PATH, "policy")

    tf_agent.train = common.function(tf_agent.train)
    
    
    collect_driver = dynamic_episode_driver.DynamicEpisodeDriver(
        train_env,
        collect_policy,
        observers=[replay_buffer.add_batch] + train_metrics,
        num_episodes=settings.COLLECT_EPISODES_PER_ITERATION,
    )
  
    iteration = 0  
  
    while iteration < settings.EPISODES:
             
        if iteration % settings.VALIDATION_EPISODE == 0:
            metrics = metric_utils.eager_compute(
                eval_metrics,
                eval_env,
                tf_agent.policy,
                num_episodes=settings.NUM_EVAL_EPISODES,
                train_step=train_step_counter,
                summary_writer=eval_summary_writer,
                summary_prefix="Metrics",
            )

            print(
                "iteration = {0}: Average Return = {1}, Average Episode Length {2}".format(
                    iteration, metrics["AverageReturn"], metrics["AverageEpisodeLength"]
                )
            )

            train_checkpointer.save(train_step_counter)
            tf_policy_saver.save(policy_dir)
            
        collect_driver.run()
        
        trajectories = replay_buffer.gather_all()
        train_loss = tf_agent.train(experience=trajectories)
        
        replay_buffer.clear()
        
        print("iteration = {0}: frame = {1} loss = {2}".format(iteration, environment_steps_metric.result(), train_loss.loss))
        
        iteration = iteration + 1
    
    metric_utils.eager_compute(
        eval_metrics,
        eval_env,
        tf_agent.policy,
        num_episodes=settings.NUM_EVAL_EPISODES,
        train_step=train_step_counter,
        summary_writer=eval_summary_writer,
        summary_prefix="Metrics",
    )

    if not os.path.exists(settings.MODEL_PATH):
        os.makedirs(settings.MODEL_PATH)

    converter = tf.lite.TFLiteConverter.from_saved_model(
        policy_dir, signature_keys=["action"]
    )
    tflite_policy = converter.convert()

    with open(os.path.join(settings.MODEL_PATH, "policy.tflite"), "wb") as f:
        f.write(tflite_policy)
        
    
    print("SUCCESS")

    sys.exit(0)


if __name__ == "__main__":
    print("Num GPUs Available: ", len(tf.config.list_physical_devices("GPU")))

    multiprocessing.handle_main(functools.partial(app.run, train))

