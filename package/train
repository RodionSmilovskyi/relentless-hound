#!/usr/bin/env python
# pylint: skip-file

import os
import sys
import settings
import argparse
import time
import tensorflow as tf

from gym.wrappers import RecordVideo
from envs.drone import FRAME_NUMBER, DroneEnv
from envs.cv_wrapper import CvWrapper
from envs.hover_reward_wrapper import HowerRewardWrapper as RewardWrapper

from tf_agents.replay_buffers import tf_uniform_replay_buffer
from tf_agents.environments.gym_wrapper import GymWrapper
from tf_agents.agents.reinforce import reinforce_agent
from tf_agents.metrics import tf_metrics
from tf_agents.eval import metric_utils
from tf_agents.drivers import dynamic_episode_driver
from tf_agents.environments import tf_py_environment
from tf_agents.networks import actor_distribution_network, value_network
from tf_agents.policies import policy_saver
from tf_agents.utils import common



def compute_avg_return(environment, policy, num_episodes=10):
    total_return = 0.0

    for _ in range(num_episodes):
        time_step = environment.reset()
        episode_return = 0.0

        while not time_step.is_last():
            action_step = policy.action(time_step)
            time_step = environment.step(action_step.action)
            episode_return += time_step.reward
        total_return += episode_return

    avg_return = total_return / num_episodes
    return avg_return.numpy()[0]


def collect_episode(environment, policy, observers, num_episodes):
    driver = dynamic_episode_driver.DynamicEpisodeDriver(
        environment,
        policy,
        observers=observers,
        num_episodes=num_episodes,
    )

    initial_time_step = environment.reset()
    driver.run(initial_time_step)


def train(use_gpu):
    train_py_env = GymWrapper(RewardWrapper(CvWrapper(DroneEnv(False))))
    train_env = tf_py_environment.TFPyEnvironment(train_py_env)
    train_step_counter = tf.Variable(0, dtype=tf.int64)

    eval_py_env = GymWrapper(
        RecordVideo(
            RewardWrapper(CvWrapper(DroneEnv(settings.USE_GUI))),
            f"{settings.OUTPUT_PATH}/data/videos",
            episode_trigger=lambda ep_id: ep_id % settings.NUM_EVAL_EPISODES == 0,
            name_prefix=f"validation-{train_step_counter.numpy()}",
        )
    )
    eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)
    eval_summary_writer = tf.compat.v2.summary.create_file_writer(
        settings.OUTPUT_PATH + "/tensorboard/eval", flush_millis=1000
    )

    actor_net = actor_distribution_network.ActorDistributionNetwork(
        train_env.observation_spec(),
        train_env.action_spec(),
        fc_layer_params=settings.FC_LAYER_PARAMS,
    )

    value_net = value_network.ValueNetwork(
        train_env.observation_spec(),
        fc_layer_params=settings.FC_LAYER_PARAMS,
    )

    optimizer = tf.keras.optimizers.Adam(learning_rate=settings.LEARNING_RATE)

    tf_agent = reinforce_agent.ReinforceAgent(
        train_env.time_step_spec(),
        train_env.action_spec(),
        actor_network=actor_net,
        value_network=value_net,
        optimizer=optimizer,
        normalize_returns=True,
        use_advantage_loss=True,
        train_step_counter=train_step_counter,
        entropy_regularization=settings.ENTROPY_RATE
    )

    tf_agent.initialize()

    eval_metrics = [
        tf_metrics.AverageReturnMetric(buffer_size=settings.NUM_EVAL_EPISODES),
        tf_metrics.AverageEpisodeLengthMetric(buffer_size=settings.NUM_EVAL_EPISODES),
    ]

    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(
        tf_agent.collect_data_spec,
        batch_size=train_env.batch_size,
        max_length=FRAME_NUMBER,
    )
    
    train_checkpointer = common.Checkpointer(
        ckpt_dir=settings.CHECKPOINT_PATH,
        max_to_keep=1,
        tf_agent=tf_agent,
        replay_buffer=replay_buffer,
    )
    train_checkpointer.initialize_or_restore()

    if train_checkpointer.checkpoint_exists == True:
        print("Checkpoint uploaded")

    tf_policy_saver = policy_saver.PolicySaver(tf_agent.policy)
    policy_dir = os.path.join(settings.OUTPUT_PATH, "policy")

    tf_agent.train = common.function(tf_agent.train)
    tf_agent.train_step_counter.assign(0)

    metric_utils.eager_compute(
        eval_metrics,
        eval_env,
        tf_agent.policy,
        num_episodes=settings.NUM_EVAL_EPISODES,
        train_step=train_step_counter,
        summary_writer=eval_summary_writer,
        summary_prefix="Metrics",
    )

    start_time = time.time()

    for _ in range(settings.EPISODES):
        collect_episode(
            train_env,
            tf_agent.collect_policy,
            [
                replay_buffer.add_batch,
                tf_metrics.AverageReturnMetric(),
                tf_metrics.AverageEpisodeLengthMetric(),
            ],
            settings.COLLECT_EPISODES_PER_ITERATION,
        )

        trajectories = replay_buffer.gather_all()
        train_loss = tf_agent.train(experience=trajectories)

        replay_buffer.clear()

        step = tf_agent.train_step_counter.numpy()
        print("step = {0}: loss = {1}".format(step, train_loss.loss))

        if step % settings.VALIDATION_EPSIODE == 0:
            metrics = metric_utils.eager_compute(
                eval_metrics,
                eval_env,
                tf_agent.policy,
                num_episodes=settings.NUM_EVAL_EPISODES,
                train_step=train_step_counter,
                summary_writer=eval_summary_writer,
                summary_prefix="Metrics",
            )

            print(
                "step = {0}: Average Return = {1}, Average Episode Length {2}".format(
                    step, metrics["AverageReturn"], metrics["AverageEpisodeLength"]
                )
            )

            train_checkpointer.save(train_step_counter)
            tf_policy_saver.save(policy_dir)


    if not os.path.exists(settings.MODEL_PATH):
        os.makedirs(settings.MODEL_PATH)

    converter = tf.lite.TFLiteConverter.from_saved_model(
        policy_dir, signature_keys=["action"]
    )
    tflite_policy = converter.convert()

    with open(os.path.join(settings.MODEL_PATH, "policy.tflite"), "wb") as f:
        f.write(tflite_policy)

    end_time = time.time()
    elapsed_time = end_time - start_time

    print(f"Elapsed time: {elapsed_time} seconds")


def parse_tuple(s):
    parts = s.split("-")
    return tuple(map(int, parts))


if __name__ == "__main__":
    parser = argparse.ArgumentParser()

    parser.add_argument("--use-gui", action="store_true")
    parser.add_argument(
        "--prefix", type=str, default=os.path.dirname(os.path.abspath(__file__))
    )
    parser.add_argument("--episodes", type=int, default=10)
    parser.add_argument("--validation-episode", type=int, default=5)
    parser.add_argument("--collect-episodes-per-iteration", type=int, default=10)
    parser.add_argument("--fc-layer-params", type=parse_tuple, default=(100, 100))
    parser.add_argument("--num-eval-episodes", type=int, default=5)
    parser.add_argument("--buffer-size", type=int, default=1000)
    parser.add_argument("--learning-rate", type=float, default=0.001)
    parser.add_argument("--entropy-rate", type=float)

    args = parser.parse_args()

    print(args)

    settings.INPUT_PATH = os.path.join(args.prefix, "input", "data")
    settings.OUTPUT_PATH = os.path.join(args.prefix, "output")
    settings.MODEL_PATH = os.path.join(args.prefix, "model")
    settings.CHECKPOINT_PATH = os.path.join(args.prefix, "checkpoints")
    settings.USE_GUI = args.use_gui
    settings.EPISODES = args.episodes
    settings.COLLECT_EPISODES_PER_ITERATION = args.collect_episodes_per_iteration
    settings.VALIDATION_EPSIODE = args.validation_episode
    settings.NUM_EVAL_EPISODES = args.num_eval_episodes
    settings.FC_LAYER_PARAMS = args.fc_layer_params
    settings.BUFFER_SIZE = args.buffer_size
    settings.LEARNING_RATE = args.learning_rate
    settings.ENTROPY_RATE = args.entropy_rate

    train_file_writer = tf.summary.create_file_writer(
        settings.OUTPUT_PATH + "/tensorboard/train", flush_millis=1000
    )
    train_file_writer.set_as_default()

    # tf.debugging.experimental.enable_dump_debug_info(
    #     settings.OUTPUT_PATH + "/tensorboard",
    #     tensor_debug_mode="FULL_HEALTH",
    #     circular_buffer_size=-1,
    # )

    print("Num GPUs Available: ", len(tf.config.list_physical_devices("GPU")))

    train(len(tf.config.list_physical_devices("GPU")) > 0)

    print("SUCCESS")

    sys.exit(0)
